{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load all necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 12)\n",
      "(2500, 11)\n"
     ]
    }
   ],
   "source": [
    "train_raw= pd.read_csv('Data/train.csv')\n",
    "test_raw= pd.read_csv('Data/test.csv')\n",
    "print(train_raw.shape)\n",
    "print(test_raw.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get rid of outliers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9323, 12)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy import stats\n",
    "train_raw=train_raw[(np.abs(stats.zscore(train_raw.drop(['Severity','Accident_ID'], axis=1))) < 3).all(axis=1)]\n",
    "\n",
    "train_raw.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declare the dependant and Independent Variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train_raw['Severity'].values\n",
    "X = train_raw.drop(['Severity','Accident_ID'], axis=1)\n",
    "X_test =test_raw.drop('Accident_ID',axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectoring the Outputs by label encoding them "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9323,)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "output_encoder=LabelEncoder()\n",
    "y=output_encoder.fit_transform(y)\n",
    "\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data Into train and Validation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8390, 10)\n",
      "(933, 10)\n",
      "(2500, 10)\n",
      "(8390,)\n",
      "(933,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train,X_val,y_train, y_val=train_test_split(X, y, test_size=0.1, random_state=0)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_val.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Safety_Score\n",
      "Days_Since_Inspection\n",
      "Total_Safety_Complaints\n",
      "Control_Metric\n",
      "Turbulence_In_gforces\n",
      "Cabin_Temperature\n",
      "Max_Elevation\n",
      "Violations\n",
      "Adverse_Weather_Metric\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "for column in X_train.columns:\n",
    "    if column !='Accident_Type_Code':\n",
    "        print(column)\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(X_train[column].values.reshape(-1,1))\n",
    "\n",
    "        # we use the fitted CountVectorizer to convert the text to vector\n",
    "        train_column=scaler.transform(X_train[column].values.reshape(-1,1))\n",
    "        val_column=scaler.transform(X_val[column].values.reshape(-1,1))\n",
    "        test_column=scaler.transform(X_test[column].values.reshape(-1,1))\n",
    "    \n",
    "        X_train[column]= train_column\n",
    "        X_val[column]= val_column\n",
    "        X_test[column]=test_column\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8390, 7)\n",
      "(933, 7)\n",
      "(2500, 7)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# integer encode\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(X_train['Accident_Type_Code'].values.reshape(-1, 1))\n",
    "\n",
    "X_train_label=label_encoder.transform(X_train['Accident_Type_Code'].values.reshape(-1, 1))\n",
    "X_val_label=label_encoder.transform(X_val['Accident_Type_Code'].values.reshape(-1, 1))\n",
    "X_test_label=label_encoder.transform(X_test['Accident_Type_Code'].values.reshape(-1, 1))\n",
    "\n",
    "# binary encode\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "X_train_label = X_train_label.reshape(len(X_train_label), 1)\n",
    "X_val_label = X_val_label.reshape(len(X_val_label), 1)\n",
    "X_test_label = X_test_label.reshape(len(X_test_label), 1)\n",
    "\n",
    "\n",
    "onehot_encoder.fit(X_train_label)\n",
    "\n",
    "X_train_one=onehot_encoder.transform(X_train_label)\n",
    "X_val_one=onehot_encoder.transform(X_val_label)\n",
    "X_test_one=onehot_encoder.transform(X_test_label)\n",
    "\n",
    "print(X_train_one.shape)\n",
    "print(X_val_one.shape)\n",
    "print(X_test_one.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8390, 17)\n",
      "(933, 17)\n",
      "(2500, 17)\n",
      "(8390,)\n",
      "(933,)\n"
     ]
    }
   ],
   "source": [
    "X_train=np.hstack((X_train,X_train_one))\n",
    "X_val=np.hstack((X_val,X_val_one))\n",
    "X_test=np.hstack((X_test,X_test_one))\n",
    "\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying Logistic Regression with HyperParameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "summary=[]\n",
    "\n",
    "#hyperparameters\n",
    "C=[0.00001,0.0001,0.001,0.01,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1,10,100,200,300,400,500,600,700,800,900,1000,10000]\n",
    "penalty=['l1','l2','elasticnet','none']\n",
    "dual=[True,False]\n",
    "fit_intercept=[True,False]\n",
    "class_weight=['balanced',None]\n",
    "solver=['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\n",
    "\n",
    "# Create hyperparameter options\n",
    "hyperparameters = dict(C=C, penalty=penalty,dual=dual,fit_intercept=fit_intercept,class_weight=class_weight,solver=solver)\n",
    "\n",
    "# create instance of model\n",
    "lr=LogisticRegression()\n",
    "\n",
    "# Create randomized search 5-fold cross validation and 100 iterations\n",
    "clf = RandomizedSearchCV(lr, hyperparameters, random_state=1, n_iter=100, cv=5, verbose=0, n_jobs=-1)\n",
    "\n",
    "# fit the data\n",
    "best_lr=clf.fit(X_train,y_train)\n",
    "\n",
    "# predict the output\n",
    "y_pred=best_lr.predict(X_val)\n",
    "\n",
    "#evaluate f1 score \n",
    "f1=f1_score(y_val,y_pred,average='weighted')\n",
    "\n",
    "#add into list to view in table \n",
    "summary.append(['logistic Regression',best_lr.best_estimator_.get_params(),f1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying SVM Regression with HyperParameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "\n",
    "C=[0.00001,0.0001,0.001,0.01,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1,10,100,200,300,400,500,600,700,800,900,1000,10000]\n",
    "penalty=['l1','l2','elasticnet']\n",
    "fit_intercept=[True,False]\n",
    "shuffle=[True,False]\n",
    "class_weight=['balanced',None]\n",
    "\n",
    "\n",
    "# Create hyperparameter options\n",
    "hyperparameters = dict(alpha=C, penalty=penalty,fit_intercept=fit_intercept,shuffle=shuffle,class_weight=class_weight)    \n",
    "\n",
    "# create instance of model\n",
    "sgd=SGDClassifier(loss='hinge')\n",
    "\n",
    "# Create randomized search 5-fold cross validation and 100 iterations\n",
    "clf = RandomizedSearchCV(sgd, hyperparameters, random_state=1, n_iter=100, cv=5, verbose=0, n_jobs=-1)\n",
    "\n",
    "# fit the data\n",
    "best_sgd=clf.fit(X_train,y_train)\n",
    "\n",
    "# predict the output\n",
    "y_pred=best_sgd.predict(X_val)\n",
    "\n",
    "#evaluate f1 score \n",
    "f1=f1_score(y_val,y_pred,average='weighted')\n",
    "\n",
    "#add into list to view in table \n",
    "summary.append(['SVM',best_sgd.best_estimator_.get_params(),f1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying KNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "\n",
    "n_neighbors=range(1,100,5)\n",
    "weights=['uniform','distance']\n",
    "algorithm=['auto', 'ball_tree', 'kd_tree','brute']\n",
    "\n",
    "\n",
    "# Create hyperparameter options\n",
    "hyperparameters = dict(n_neighbors=n_neighbors, weights=weights,algorithm=algorithm)    \n",
    "\n",
    "# create instance of model\n",
    "knn=KNeighborsClassifier()\n",
    "\n",
    "# Create randomized search 5-fold cross validation and 100 iterations\n",
    "clf = RandomizedSearchCV(knn, hyperparameters, random_state=1, n_iter=100, cv=5, verbose=0, n_jobs=-1)\n",
    "\n",
    "# fit the data\n",
    "best_knn=clf.fit(X_train,y_train)\n",
    "\n",
    "# predict the output\n",
    "y_pred=best_knn.predict(X_val)\n",
    "\n",
    "#evaluate f1 score \n",
    "f1=f1_score(y_val,y_pred,average='weighted')\n",
    "\n",
    "#add into list to view in table \n",
    "summary.append(['knn',best_knn.best_estimator_.get_params(),f1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying Descision Tree Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "min_samples_split=range(2,100,5)\n",
    "min_samples_leaf=range(1,100,5)\n",
    "criterion = ['gini', 'entropy']\n",
    "splitter=['best', 'random']\n",
    "max_features=['auto', 'sqrt', 'log2']\n",
    "class_weight=['balanced',None]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create hyperparameter options\n",
    "hyperparameters = dict(min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf,criterion=criterion,splitter=splitter,max_features=max_features,class_weight=class_weight)    \n",
    "\n",
    "# create instance of model\n",
    "DT=DecisionTreeClassifier()\n",
    "\n",
    "# Create randomized search 5-fold cross validation and 100 iterations\n",
    "clf = RandomizedSearchCV(DT, hyperparameters, random_state=1, n_iter=100, cv=5, verbose=0, n_jobs=-1)\n",
    "\n",
    "# fit the data\n",
    "best_dt=clf.fit(X_train,y_train)\n",
    "\n",
    "# predict the output\n",
    "y_pred=best_dt.predict(X_val)\n",
    "\n",
    "#evaluate f1 score \n",
    "f1=f1_score(y_val,y_pred,average='weighted')\n",
    "\n",
    "#add into list to view in table \n",
    "summary.append(['DT',best_dt.best_estimator_.get_params(),f1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying Random Forest  Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "min_samples_split=range(2,100,5)\n",
    "min_samples_leaf=range(1,100,5)\n",
    "criterion = ['gini', 'entropy']\n",
    "n_estimators=range(1,1000,25)\n",
    "bootstrap=[True,False]\n",
    "class_weight=['balanced',None]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create hyperparameter options\n",
    "hyperparameters = dict(min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf,criterion=criterion,\n",
    "                       n_estimators=n_estimators,bootstrap=bootstrap,class_weight=class_weight)    \n",
    "\n",
    "# create instance of model\n",
    "RF=RandomForestClassifier()\n",
    "\n",
    "# Create randomized search 5-fold cross validation and 100 iterations\n",
    "clf = RandomizedSearchCV(RF, hyperparameters, random_state=1, n_iter=100, cv=5, verbose=0, n_jobs=-1)\n",
    "\n",
    "# fit the data\n",
    "best_rf=clf.fit(X_train,y_train)\n",
    "\n",
    "# predict the output\n",
    "y_pred=best_rf.predict(X_val)\n",
    "\n",
    "#evaluate f1 score \n",
    "f1=f1_score(y_val,y_pred,average='weighted')\n",
    "\n",
    "#add into list to view in table \n",
    "summary.append(['RF',best_rf.best_estimator_.get_params(),f1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying Gradient boosted Descision Tree Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "max_depth=range(2,100,5)\n",
    "learning_rate =np.arange(0,1,0.1)\n",
    "n_estimators=range(1,1000,25)\n",
    "min_child_weight = [ 1, 3, 5, 7 ]\n",
    "gamma  = [ 0.0, 0.1, 0.2 , 0.3, 0.4 ]\n",
    "colsample_bytree = [ 0.3, 0.4, 0.5 , 0.7 ]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create hyperparameter options\n",
    "hyperparameters = dict(max_depth=max_depth, learning_rate=learning_rate,n_estimators=n_estimators,min_child_weight=min_child_weight,\n",
    "                      gamma=gamma,colsample_bytree=colsample_bytree)    \n",
    "\n",
    "# create instance of model\n",
    "GBDT=XGBClassifier()\n",
    "\n",
    "# Create randomized search 5-fold cross validation and 100 iterations\n",
    "clf = RandomizedSearchCV(GBDT, hyperparameters, random_state=1, n_iter=100, cv=5, verbose=0, n_jobs=-1)\n",
    "\n",
    "# fit the data\n",
    "best_gb=clf.fit(X_train,y_train)\n",
    "\n",
    "# predict the output\n",
    "y_pred=best_gb.predict(X_val)\n",
    "\n",
    "#evaluate f1 score \n",
    "f1=f1_score(y_val,y_pred,average='weighted')\n",
    "\n",
    "#add into list to view in table \n",
    "summary.append(['XGDT',best_gb.best_estimator_.get_params(),f1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result=pd.DataFrame(summary,columns=['Model','Best Hyper parameters','f1-score'])\n",
    "result.to_csv('model_result.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Best Hyper parameters</th>\n",
       "      <th>f1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>logistic Regression</td>\n",
       "      <td>{'C': 800, 'class_weight': 'balanced', 'dual':...</td>\n",
       "      <td>0.647304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SVM</td>\n",
       "      <td>{'alpha': 0.001, 'average': False, 'class_weig...</td>\n",
       "      <td>0.631023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>knn</td>\n",
       "      <td>{'algorithm': 'brute', 'leaf_size': 30, 'metri...</td>\n",
       "      <td>0.721164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DT</td>\n",
       "      <td>{'ccp_alpha': 0.0, 'class_weight': None, 'crit...</td>\n",
       "      <td>0.828199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RF</td>\n",
       "      <td>{'bootstrap': False, 'ccp_alpha': 0.0, 'class_...</td>\n",
       "      <td>0.906947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>XGDT</td>\n",
       "      <td>{'base_score': 0.5, 'booster': 'gbtree', 'cols...</td>\n",
       "      <td>0.965696</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Model                              Best Hyper parameters  \\\n",
       "0  logistic Regression  {'C': 800, 'class_weight': 'balanced', 'dual':...   \n",
       "1                  SVM  {'alpha': 0.001, 'average': False, 'class_weig...   \n",
       "2                  knn  {'algorithm': 'brute', 'leaf_size': 30, 'metri...   \n",
       "3                   DT  {'ccp_alpha': 0.0, 'class_weight': None, 'crit...   \n",
       "4                   RF  {'bootstrap': False, 'ccp_alpha': 0.0, 'class_...   \n",
       "5                 XGDT  {'base_score': 0.5, 'booster': 'gbtree', 'cols...   \n",
       "\n",
       "   f1-score  \n",
       "0  0.647304  \n",
       "1  0.631023  \n",
       "2  0.721164  \n",
       "3  0.828199  \n",
       "4  0.906947  \n",
       "5  0.965696  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## saving DT,RF and XGDT models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump(best_gb,open('xgboost_96.pkl','wb'))\n",
    "\n",
    "\n",
    "# # load the model from disk\n",
    "# loaded_model = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## predict with the best model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_gb.fit(X_train,y_train)\n",
    "y_pred=best_gb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "output=pd.DataFrame(test_raw['Accident_ID'],columns=['Accident_ID'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "output['Severity']=list(output_encoder.inverse_transform(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accident_ID</th>\n",
       "      <th>Severity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Highly_Fatal_And_Damaging</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>Significant_Damage_And_Fatalities</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14</td>\n",
       "      <td>Significant_Damage_And_Serious_Injuries</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17</td>\n",
       "      <td>Highly_Fatal_And_Damaging</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21</td>\n",
       "      <td>Significant_Damage_And_Fatalities</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2495</th>\n",
       "      <td>12484</td>\n",
       "      <td>Highly_Fatal_And_Damaging</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2496</th>\n",
       "      <td>12487</td>\n",
       "      <td>Significant_Damage_And_Serious_Injuries</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2497</th>\n",
       "      <td>12488</td>\n",
       "      <td>Significant_Damage_And_Serious_Injuries</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2498</th>\n",
       "      <td>12491</td>\n",
       "      <td>Significant_Damage_And_Serious_Injuries</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2499</th>\n",
       "      <td>12493</td>\n",
       "      <td>Highly_Fatal_And_Damaging</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2500 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Accident_ID                                 Severity\n",
       "0               1                Highly_Fatal_And_Damaging\n",
       "1              10        Significant_Damage_And_Fatalities\n",
       "2              14  Significant_Damage_And_Serious_Injuries\n",
       "3              17                Highly_Fatal_And_Damaging\n",
       "4              21        Significant_Damage_And_Fatalities\n",
       "...           ...                                      ...\n",
       "2495        12484                Highly_Fatal_And_Damaging\n",
       "2496        12487  Significant_Damage_And_Serious_Injuries\n",
       "2497        12488  Significant_Damage_And_Serious_Injuries\n",
       "2498        12491  Significant_Damage_And_Serious_Injuries\n",
       "2499        12493                Highly_Fatal_And_Damaging\n",
       "\n",
       "[2500 rows x 2 columns]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.to_csv('Result.csv',index=False)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
